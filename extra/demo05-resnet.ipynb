{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0c76426af416489dbdfef60cf05c94a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_867ff8017fe14659a0304b6b14ff0f6f",
              "IPY_MODEL_17bcdb9fbf0f4687b249823e993d4038",
              "IPY_MODEL_df6a5f01e43841dd9f41c7097cb5818e"
            ],
            "layout": "IPY_MODEL_539a6582197444769a4b94e262504abf"
          }
        },
        "867ff8017fe14659a0304b6b14ff0f6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a160b68ffa84c7c9e3795e74444572c",
            "placeholder": "​",
            "style": "IPY_MODEL_5d3c68a04c9b4ea298ae8c9bb329704d",
            "value": "100%"
          }
        },
        "17bcdb9fbf0f4687b249823e993d4038": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e74b24ea1934b068120898f32e3477c",
            "max": 87319819,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ce030af28efb40358d43a52d462466ff",
            "value": 87319819
          }
        },
        "df6a5f01e43841dd9f41c7097cb5818e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45ab018f72904877a64397ad1ad8302b",
            "placeholder": "​",
            "style": "IPY_MODEL_d331782382a542e4b627b6b17020a84a",
            "value": " 83.3M/83.3M [00:00&lt;00:00, 95.2MB/s]"
          }
        },
        "539a6582197444769a4b94e262504abf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a160b68ffa84c7c9e3795e74444572c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d3c68a04c9b4ea298ae8c9bb329704d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e74b24ea1934b068120898f32e3477c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce030af28efb40358d43a52d462466ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "45ab018f72904877a64397ad1ad8302b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d331782382a542e4b627b6b17020a84a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rtealwitter/dl-demos/blob/main/demo05-resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc9wyyGloQwL"
      },
      "source": [
        "# Finetuning a ResNet\n",
        "\n",
        "Modern deep convnets tend to have tens (if not hundreds) of layers, with millions (if not tens of millions) of trainable parameters. More often than not several of these layers have skipped connections; the *ResNet* family of networks are an example. \n",
        "\n",
        "The flip side of having such deep network architectures is that to properly learn such networks, one requires *massive* amounts of training data. In most applications, access to such massive datasets simply isn't available; gathering and curating a dataset with a few hundred/thousand examples itself can be a challenge.\n",
        "\n",
        "What should one do in the \"small data\" setting? A possible solution:\n",
        "* start with a deep network architecture initialized with *pre-trained* weights, and\n",
        "* *fine-tune* the network weights on the (small) training dataset.\n",
        "\n",
        "In this demo we will see how to train a simple cat-vs-dog classifier using a very small training dataset of 60 images. The dataset is provided [here](https://github.com/chinmayhegde/dl-demos/blob/main/data.zip). You can unzip and save the dataset anywhere you like; I've saved it to my Google drive folder: `MyDrive/data`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGT3H4DjoO9n"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since I put the data in `MyDrive/data`, I'll have to give colab access to the folder so we can load it."
      ],
      "metadata": {
        "id": "0UQfZh_XFrpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pMIhxViRFP1s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2de281e2-0113-4559-e96e-14c0c986014c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNHnEAckpqps"
      },
      "source": [
        "\n",
        "# data transforms\n",
        "dset_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])])\n",
        "\n",
        "\n",
        "# Use the image folder function to create datasets\n",
        "dsets = {x: datasets.ImageFolder(f\"/content/drive/MyDrive/data/{x}\", dset_transform)\n",
        "         for x in ['train', 'val']}\n",
        "\n",
        "# create data loader\n",
        "dataloaders = {x: torch.utils.data.DataLoader(dsets[x], batch_size=16,\n",
        "                                              shuffle=(x == \"train\"))\n",
        "               for x in ['train', 'val']}\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYSEZtTDJUYV"
      },
      "source": [
        "# Loading a pre-trained ResNet model\n",
        "\n",
        "Let's load a ResNet34 model from `torchvision` and examine it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOYRfm1pBcoM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "0c76426af416489dbdfef60cf05c94a3",
            "867ff8017fe14659a0304b6b14ff0f6f",
            "17bcdb9fbf0f4687b249823e993d4038",
            "df6a5f01e43841dd9f41c7097cb5818e",
            "539a6582197444769a4b94e262504abf",
            "7a160b68ffa84c7c9e3795e74444572c",
            "5d3c68a04c9b4ea298ae8c9bb329704d",
            "9e74b24ea1934b068120898f32e3477c",
            "ce030af28efb40358d43a52d462466ff",
            "45ab018f72904877a64397ad1ad8302b",
            "d331782382a542e4b627b6b17020a84a"
          ]
        },
        "outputId": "87ee3cbe-2766-439e-ae9c-3e536377b8ae"
      },
      "source": [
        "# intialize model\n",
        "model = models.resnet34(pretrained=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/83.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c76426af416489dbdfef60cf05c94a3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVgCL8ZTtvTt",
        "outputId": "f9e658f0-bf12-4017-edd0-3b3e21edb786"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (4): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (5): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfWGLTlJLxzI"
      },
      "source": [
        "Hmm, lots of layers. Each `BasicBlock` is two or three conv layers with a skipped connection, with batch-norm layers thrown in for good measure. Several such residual blocks are pieced together, and in the end there is a dense layer with 1000 output neurons. This model has been trained on the well-known *ImageNet* dataset (which has over a million images with 1000 classes). As an aside, let's examine the number of trainable parameters in the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jl9m-Z4EDFFW",
        "outputId": "6bf4a5b5-a75b-443a-d84f-d3fc7b4f5e63"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(count_parameters(model))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21797672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdzbKTClM_Qv"
      },
      "source": [
        "Let us finetune this model for our cat-vs-dog classification problem. Since this is a binary classifier we will redefine the output (linear) layer to have 2 outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qkjny7j5BDRS",
        "outputId": "13a34bee-d329-4fb0-9d6f-1c60cc399cd2"
      },
      "source": [
        "num_ftrs = model.fc.in_features\n",
        "print(num_ftrs)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhnOEfZoDQS6"
      },
      "source": [
        "model.fc = nn.Linear(num_ftrs, 2)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv86CyS3DVe8",
        "outputId": "13f17a0d-3493-4c6e-d3d1-461d4c515eec"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (4): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (5): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIDpqRLJOMaJ"
      },
      "source": [
        "Observe now that the basic ResNet34 backbone remains the same; only the output layer has changed. In fact, all of the weights (except the output layer) also have been retained.  \n",
        "\n",
        "Let's do a quick model evaluation to check if there are any errors thrown during prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bq72K2tuDn3h",
        "outputId": "e70dd729-382d-4b62-9e08-9c657d992f38"
      },
      "source": [
        "model.eval()\n",
        "corrects = 0\n",
        "for batch_idx, (inputs,labels) in enumerate(dataloaders['val'], 1):\n",
        "  with torch.set_grad_enabled(False):\n",
        "    outputs = model(inputs)\n",
        "    _, preds = torch.max(outputs,1)\n",
        "    \n",
        "  corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "print(corrects.float() / len(dataloaders['val'].dataset))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4583)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI3eN9vcPEHb"
      },
      "source": [
        "As we can see, we get an accuracy of 50\\% on the test set -- which is exactly what we would expect since the weights of the output layer are random. \n",
        "\n",
        "We are now ready to start fine-tuning! The rest of the code below is boilerplate training; we see below that only a few epochs are enough to tune the weights to our training dataset.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZ8n_358A6aY"
      },
      "source": [
        "# define loss function, optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "save_loss = {'train':[], 'val':[]}\n",
        "save_acc = {'train':[], 'val':[]}"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJHYjaYtrNHm",
        "outputId": "e713c410-f355-4f09-810a-92a8b7fe0f83"
      },
      "source": [
        "for epoch in range(5):\n",
        "\n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in ['train', 'val']:\n",
        "        if phase == 'train':\n",
        "            model.train()  # Set model to training mode\n",
        "        else:\n",
        "            model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        current_loss = 0.0\n",
        "        current_corrects = 0\n",
        "\n",
        "        for batch_idx, (inputs, labels) in enumerate(dataloaders[phase], 1):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Time to carry out the forward training poss\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # backward + optimize only if in training phase\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            # We want variables to hold the loss/acc statistics\n",
        "            current_loss += loss.item() * inputs.size(0)\n",
        "            current_corrects += torch.sum(preds == labels.data)\n",
        "        # saving variable for plottin\n",
        "        save_loss[phase] += [current_loss / len(dataloaders[phase].dataset)]\n",
        "        save_acc[phase] += [current_corrects.float() / len(dataloaders[phase].dataset)]\n",
        "\n",
        "        # pretty print\n",
        "        print(f\"Epoch:{epoch} -- Phase:{phase} -- Loss:{save_loss[phase][-1]:.2f} -- Acc:{save_acc[phase][-1]*100:.2f}\")\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:0 -- Phase:train -- Loss:0.44 -- Acc:90.00\n",
            "Epoch:0 -- Phase:val -- Loss:0.34 -- Acc:91.67\n",
            "Epoch:1 -- Phase:train -- Loss:0.29 -- Acc:98.33\n",
            "Epoch:1 -- Phase:val -- Loss:0.18 -- Acc:100.00\n",
            "Epoch:2 -- Phase:train -- Loss:0.14 -- Acc:100.00\n",
            "Epoch:2 -- Phase:val -- Loss:0.10 -- Acc:100.00\n",
            "Epoch:3 -- Phase:train -- Loss:0.09 -- Acc:100.00\n",
            "Epoch:3 -- Phase:val -- Loss:0.05 -- Acc:100.00\n",
            "Epoch:4 -- Phase:train -- Loss:0.03 -- Acc:100.00\n",
            "Epoch:4 -- Phase:val -- Loss:0.04 -- Acc:100.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "JGgbCwdTuy1h",
        "outputId": "70ee7fc9-f92f-492e-bb78-8f4e9f4107a9"
      },
      "source": [
        "plt.plot(save_acc['train'])\n",
        "plt.plot(save_acc['val'])\n",
        "plt.legend([\"train\", \"test\"])\n",
        "plt.title(\"Accuracy\")\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Accuracy')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1__s46hPeUg"
      },
      "source": [
        "There we go! That took only 4 epochs of finetuning.\n",
        "\n",
        "This was a very small dataset (of only 60 training images) so it is not that surprising that we were able to fit the data so easily. Try training your own classifier with a slightly larger set of data points, and see if you can get similar results."
      ]
    }
  ]
}